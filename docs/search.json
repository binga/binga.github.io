[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Phani Srikanth",
    "section": "",
    "text": "testing."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to my home!",
    "section": "",
    "text": "üî≠ I‚Äôm currently working on helping my teammates succeed in protecting users from malicious file-based and web-based attacks using Machine Learning at Microsoft.\nüå± I‚Äôm learning how to lead great teams!\nüí¨ Ask me about Data Science / Building Career Paths / Storytelling / Interesting People on Internet / Systems Thinking.\nüì´ How to reach me: Twitter / LinkedIn / Microsoft\nüòÑ Pronouns: He/Him.\n‚ö° Fun fact: I never uttered a word until I turned 3.\n\nRead more about me here."
  },
  {
    "objectID": "recommendations.html",
    "href": "recommendations.html",
    "title": "Recommendations",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\nSports\n\n\nChatGPT\n\n\nLLMs\n\n\n\n\n\n\n\nFeb 21, 2023\n\n\n\n\n\n\n\n\n\n\n¬†\n\n\n\n\n4 min\n\n\n\nData Science\n\n\nCompetitions\n\n\nHackathons\n\n\nMachine Learning\n\n\nAnalytics Vidhya\n\n\n2017\n\n\n\n\n\n\n\nMar 30, 2017\n\n\n\n\n\n\n\n\n\n\n¬†\n\n\n\n\n2 min\n\n\n\nData Science\n\n\nCompetitions\n\n\nHackathons\n\n\nMachine Learning\n\n\nHackerEarth\n\n\n2016\n\n\n\n\n\n\n\nDec 16, 2016\n\n\n\n\n\n\n\n\n\n\n¬†\n\n\n\n\n4 min\n\n\n\nData Science\n\n\nCompetitions\n\n\nHackathons\n\n\nMachine Learning\n\n\nAnalytics Vidhya\n\n\n2015\n\n\n\n\n\n\n\nSep 15, 2015\n\n\n\n\n\n\n\n\n\n\n¬†\n\n\n\n\n2 min\n\n\n\nData Science\n\n\nCompetitions\n\n\nHackathons\n\n\nMachine Learning\n\n\n2015\n\n\n\n\n\n\n\nAug 15, 2015\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2016-12-16-Winning_HackerEarth_ML_Challenge/index.html",
    "href": "posts/2016-12-16-Winning_HackerEarth_ML_Challenge/index.html",
    "title": "Winning the HackerEarth Machine Learning challenge",
    "section": "",
    "text": "Societe Generale, one of the largest banks in France, in collaboration with HackerEarth, organised Brainwaves, the annual hackathon at Bengaluru on November 12‚Äì13, 2016. The theme of the hackathon this year was ‚ÄúMachine Learning‚Äù. The hackathon had an online qualifier from where 85 top teams out of 2200 registrations from all over India, were selected for the final round. The final round was a 30-hour long hackathon which needed the teams to solve 1 problem out of 3 given problems spanning across transaction fraud detection, image and text analytics. I decided to solve the former since I have had experience working with banking data in multiple firms I have previously worked with."
  },
  {
    "objectID": "posts/2016-12-16-Winning_HackerEarth_ML_Challenge/index.html#brief-approach",
    "href": "posts/2016-12-16-Winning_HackerEarth_ML_Challenge/index.html#brief-approach",
    "title": "Winning the HackerEarth Machine Learning challenge",
    "section": "Brief Approach",
    "text": "Brief Approach\nFor the first problem, we were given millions of historical transactions to find patterns from and use these patterns to find anomalies on future transactions. We quickly skimmed through the data and built our machine learning model to predict the fraud on future transactional data and ranked #1 on the leaderboard. Eventually, we also built dashboards which can be used for proactive real-time monitoring for detection of any kind of new anomalies, or they can also be used to monitor transaction throughput etc. You could think of it like a one stop control centre with a global view of what‚Äôs going through the system. One commonly known fraudulent behaviour is, fraudsters try to exploit the system by doing high number of small debits and one large credit, thus swindling the money across countries and exchanges and hence trying to circumvent the defences of the system. This particular kind was quite challenging to incorporate into our machine learning model on and we are glad to have solved it to a good extent in 30 hours. Eventually, we had a good dashboard, a very good model and made an excellent pitch to the jury and ranked 1st amongst 85 teams."
  },
  {
    "objectID": "posts/2016-12-16-Winning_HackerEarth_ML_Challenge/index.html#experiences-at-the-hackathon",
    "href": "posts/2016-12-16-Winning_HackerEarth_ML_Challenge/index.html#experiences-at-the-hackathon",
    "title": "Winning the HackerEarth Machine Learning challenge",
    "section": "Experiences at the hackathon",
    "text": "Experiences at the hackathon\nThe hackathon was very well organised in terms of the quality of problem statements in the online and offline rounds, the way the organising team responded to any queries. It was genuinely surprising to see many mentors walking down to our table, talking to us about our backgrounds and providing us various domain related insights which augmented our model and resulted in higher performance. Even during the late hours none of them really left the place, they would always come and check if we were stalled anywhere and help us directionally so that we make constant progress. Having participated in a lot of hackathons prior to this one, I am very surprised by the energy levels of the mentors at Societe Generale.\nTo conclude, I would like to thank HackerEarth, Societe Generale, mentors and most importantly Phani Srinath and Supreeth Manyam for their fantastic work during the weekend. Great work guys! If not for any of the above, I am sure that weekend wouldn‚Äôt have been so memorable.\nAnd yes‚Ä¶ we partied long and hard that night! :-)\n  \nOur team with the amazing mentors and Societe Generale India CEO\nTop 3 teams pose for the customary picture. Overall, November 2016 has been a great month since Supreeth Manyam and I have also ended up winning another machine learning contest on CrowdAnalytix. We participated with the moniker Popeye For Olive!\nThat‚Äôs it for now. Stay tuned‚Ä¶"
  },
  {
    "objectID": "posts/2017-03-30-Winning_Two_ML_Challenges/index.html",
    "href": "posts/2017-03-30-Winning_Two_ML_Challenges/index.html",
    "title": "Winning two Machine Learning Challenges in the same month",
    "section": "",
    "text": "February 2017 has been a really good month since I have had an opportunity to work on multiple machine learning challenges, hosted by Analytics Vidhya, and came out winning both of them. I wanted to share my experiences of participating in these challenges."
  },
  {
    "objectID": "posts/2015-08-18-DataScience_Hackathon_Datameet/index.html",
    "href": "posts/2015-08-18-DataScience_Hackathon_Datameet/index.html",
    "title": "Data Science Hackathon ‚Äî DataMeet Mumbai",
    "section": "",
    "text": "Problem Statement: Propensity model development ‚Äìthe client has a couple of use cases where they have not been able to get 80% response capture in top 3 deciles / >3X lift in the top decile ‚Äî inspite of several iterations. The expectation here would be identification of any new technique / algorithm (apart from logistic regression), which can help get the desired model performance.\nVarious kinds of customer data was given to us for utilizing in the data analysis. We were asked to figure out which customers are potential loan takers. The data given to us had a train set and a test set, total amounting to 8 lakh records and various features of each of the customers. The evaluation metric seemed trickier at first. I have often worked with logloss and AUC for binary classification problems. But, this problem was aspiring for higher lift in the top 10% and 30% recommendations.\nWe initially started with linear models and observed that the lift we were able to achieve was worser than the benchmark. Therefore, we switched to nonlinear models like decision trees and ensembles like Random Forest. Forests were able to produce much better results. But, due to the inherent nature of forests, the probabilities aren‚Äôt uniformly calibrated. So, we had to choose an algorithm that directly optimized the evaluation metric and thus we chose XGBoost (which is by far the most popular tool on Kaggle) which is fast and optimized the right evaluation metric and we observed that the XGBoost was performing way better than forests. As the requirement from the organizers included fast computation and cross-platform support, we stuck with XGBoost instead of creating complex ensembles which are engineering-heavy, and choose to take the feature creation route.\n\nOne of our teammates focused on creation of various features while the others focused on modeling and pushing the accuracy. We were pretty happy to obtain a lift of 5.5x while the benchmark was around 3x. Thus we chose to stick with 1 model!\nOn Sunday, we met the organizers and they were pretty happy with the model performance and they gave us 5 more days to increase the accuracy. But, we could only push it by 1% considering the heavy engineering effort required to make a significant progress.\nAfter 2 weeks of evaluation, I received a mail today from them saying, we stood 2nd on the competition from the total pool of 15 teams which are from various parts of India and from reputed universities like CMU etc.\nI‚Äôm pretty happy with the 2nd position finish as this is the first time I ended up winning a Data Science Challenge.\nI hope there are many more to come!"
  },
  {
    "objectID": "posts/2015-09-15-AnalyticsVidhya_3.X_Hackathon/index.html",
    "href": "posts/2015-09-15-AnalyticsVidhya_3.X_Hackathon/index.html",
    "title": "Analytics Vidhya 3.X Hackathon",
    "section": "",
    "text": "Problem Statement: Digital arms of banks today face challenges with lead conversion, they source leads through mediums like search, display, email campaigns and via affiliate partners. Here Happy Customer Bank faces same challenge of low conversion ratio. They have given a problem to identify the customers segments having higher conversion ratio for a specific loan product so that they can specifically target these customers.\nIt was eerily similar to a problem statement I‚Äôve worked with, 2 weeks earlier to this competition. I quickly jumped onto the dataset and noticed that the dataset was quite small and it was easy to run several experiments over the stipulated 2 days of time.\nThe evaluation metric of this competition was AUC score. So, I once went through the ‚Äúrank averaging‚Äù section on this awesome ensembling guide. This article should be read multiple times by any data analyst participating in a data science competition. Also, check out my 5 minutes to understand AUC guide I posted on Kaggle forums.\nAs a first step, I established the pipeline ‚Äî reading data, extracting a few basic features, building a simple classifier, generating the predictions file in a couple of hours and then went through the feature extraction as the time progressed. As it was a 2-day hackathon, we had to use the best tools available and snatch the lead. Once again, linear models failed to get me a good score and models like Random Forest worked well. As expected, XGBoost again did phenomenally well on the dataset.\nThe crucial points I noted while I worked on this dataset were: 1. When a high-cardinality categorical variable like ‚Äúcity‚Äù or ‚Äústate‚Äù is given, try to use it. Also, merge all rare levels. Really helps! 2. XGBoost has a way to treat missing values. Sometimes it‚Äôs better to leave the missing values as-is and allow the algorithm to take a decision on how to deal with them. 3. For Random Forests, I treated the missing values as -3.14, just as some other level. 4. Use dates whenever given. You could extract day, day of week, month, quarter, year and do forward subset selection for a start!\nIn the beginning, I was leading on the leaderboard but soon, others like phani_srinath have taken an insane lead on the LB. I was lucky to figure out my mistake in tuning the XGB and I could improve my score. Somehow, I was unable to get my Random Forest to work well and I was out of tricks. Then came in the defining moment. I tried FTRL, an online linear model (an algorithm Google uses in its ad-prediction engine) and it worked! This came in as a shocking/pleasant surprise and I did weighted ensembling. Inspite of all this, I ended up 2nd on the private leaderboard, losing by a tiny 0.00032., though, I was happy that I lost to an incredibly tough competitor.\n\nPhani Srinath finished 1st and I marginally missed out on the weekend contest! Fortunately, a few participants asked the organizers to extend the competition for a few more days and they obliged and extended it for 5 more days. This time I wanted to make sure I give my best shot. To demonstrate how close was the competition, the organizers revealed just the top 2 private scores before the 2nd leg of the competition and this gave me and phani_srinath an unfair lead. So, we gave out our approach to all the participants on the forum. And everybody picked it up so well that I ended up 10th on the public leaderboard in the weeklong competition. This time I built 10XGB models, 5 RF models, 5 FTRL models, averaged them for stability and made sure I ensemble my models well and made a gamble. Fortunately, it worked and Yayy! I finished highest on the private leaderboard of the weeklong competition and secured 1st position out of ~60 odd participants! I was only 50% sure about this because my CV scores were pretty strong and I used ensembling, way too much, to effectively fight variance in the dataset.\nThis time I finished 1st and Phani Srinath finished 3rd on the weeklong contest.\n\nI also gave out a small guide, how to effectively cross-validate with XGBoost. See here!\nI am happy that I secured 2 podium finishes and 1 win in this hackathon. This makes it 2 prizes in a month. The previous one was this. Very lucky 30 days it has been!\nFor other details, check out my codes on github!\nHopefully, I continue to learn more and more, and do well on future competitions. Game on!\nFinally, a shout-out to the AWESOME ‚ÄòT‚Äô trio,\n\nTianqi ‚Äî creator of XGBoost\nTriskelion ‚Äî for his brilliant ensembling guide\nTinrtgu ‚Äî he deserves an oscar for his beautiful online-lr FTRL code!"
  },
  {
    "objectID": "posts/2017-03-30-Winning_Two_ML_Challenges/index.html#analytics-roadshow-minihack",
    "href": "posts/2017-03-30-Winning_Two_ML_Challenges/index.html#analytics-roadshow-minihack",
    "title": "Winning two Machine Learning Challenges in the same month",
    "section": "Analytics Roadshow MiniHack",
    "text": "Analytics Roadshow MiniHack\nThe first of these challenges was a mini-hack in which participants had three hours of time to build the most accurate predictive model that predicts surge pricing type for a taxi aggregator so that the matchmaking between the right cabs and right customers is quick and efficient.\nAfter a quick exploration of data, I encoded the Gender, Type of Cab and Confidence Life Style Index to numerical values so that tree models could consume the data. A 3-hour challenge doesn‚Äôt give you too much room to start with feature engineering. Hence, I directly jumped to building models to quickly test what works and what doesn‚Äôt. Linear models and Random forests produced around 0.63 and 0.68 on both cross-validation and leaderboard respectively. However, these stood around top 30th percentile. XGBoost, once again outperformed other models and it put me one among top 5 on leaderboard within an hour of the challenge. However, it was difficult to make any improvements to this.\n\nWhat didn‚Äôt work\n2-way variable interactions especially between var1, var2, var3 didn‚Äôt improve the score. Feature selection didn‚Äôt add any value. Ensembling RFs with XGBs produced no good effect. In a lot of ways, I was stuck for a long time. 30 minutes to finish line. I dropped to 15th position among 170 participants.\n\n\nWhat worked\nAs ensembling wasn‚Äôt working, I focused on improving my XGB model. I changed the min_child_weight parameter and voila! my score shot up. I was totally underfitting all the time, damn. I went up by 8 places. 15 minutes to go, I quickly lowered my learning_rate, my cross-validation score went up and it scored 2nd on LB! Falling short of time, I couldn‚Äôt add any other model. I was sure I didn‚Äôt overfit and I was delighted to see that my model was one of the most stable models on public and private and I scored 1st on private leaderboard.\nMaybe, it ain‚Äôt over until it‚Äôs over.\nRef: https://datahack.analyticsvidhya.com/contest/minihack-machine-learning/lb\nSource code here: https://github.com/binga/AnalyticsVidhya_Minihack\n2 weekends later.."
  },
  {
    "objectID": "posts/2023-02-21-F1_Prompts/index.html",
    "href": "posts/2023-02-21-F1_Prompts/index.html",
    "title": "Can ChatGPT generate Accurate Sports Statistics?",
    "section": "",
    "text": "Going broad\nTo test the limits of ChatGPT, I decided to go broad and ask ‚ÄúWhat is the total number of pit stops taken by Lewis Hamilton in Formula 1 2020 season?‚Äù\n\n\n\nPitstop data\n\n\nChatGPT gets this incorrect. There is no way that Lewis Hamilton took 5 pitstops in the entire season. So, instead of going this approach, I decided to go with a more specific question.\n\n\nStart narrow and build slowly.\n\nStart with a simple question\nI asked ‚ÄúShow a table with names of each race in f1 2020 season?‚Äù\n\n\n\nRace data\n\n\nA quick look at wikipedia (https://en.wikipedia.org/wiki/2020_Formula_One_World_Championship) shows that this is accurate. This is a good step. Now, let‚Äôs add race-wise pitstop data to the table.\n\n\nAdd details\n\n\n\nRace data by pitstops\n\n\nOn the first look, this looks good. But, there is a problem. While the model generated reasonable statistics, they are not accurate. A quick check at Austrian Grand Prix and Styrian Grand Prix confirms that Lewis did not take 2 & 3 pitstops during the race.\nWhile this certainly looks like progress, we still have some way to go with factual accuracy. Let‚Äôs try a simpler question, instead.\n\n\nAdd more details\nI asked ChatGPT to add a column where Lewis finished each race.\n\n\n\nRace data with pitstops and finishing position\n\n\nA cursorily glance at the table shows that the model is getting the finishing position correct. But, the positions data is incorrect for 2 races. I‚Äôll leave it to you to figure out which ones! :-)\n\n\n\nTakeaways\n\nWithout any coding and referencing multiple websites, I was able to pull this data. So, large language models like ChatGPT will be super useful in a wide variety of ways.\nWe still do not know the most appropriate ways to ask the model for the answer. We are figuring it out as we go.\nThe model is still not perfect. It is still making mistakes. But, it is getting better with time."
  },
  {
    "objectID": "posts/2023-02-21-F1_Prompts/index.html#going-broad",
    "href": "posts/2023-02-21-F1_Prompts/index.html#going-broad",
    "title": "Generating sports statistics using ChatGPT",
    "section": "Going broad",
    "text": "Going broad\nTo test the limits of ChatGPT, I decided to go broad and ask ‚ÄúWhat is the total number of pit stops taken by Lewis Hamilton in Formula 1 2020 season?‚Äù\n\n\n\n\nPitstop data\n\n\nChatGPT gets this incorrect. There is no way that Lewis Hamilton took 5 pitstops in the entire season. So, instead of going this approach, I decided to go with a more specific question."
  },
  {
    "objectID": "posts/2023-02-21-F1_Prompts/index.html#start-going-narrow-and-build-it-slowly.",
    "href": "posts/2023-02-21-F1_Prompts/index.html#start-going-narrow-and-build-it-slowly.",
    "title": "Generating sports statistics using ChatGPT",
    "section": "Start going narrow and build it slowly.",
    "text": "Start going narrow and build it slowly.\n\nStart with a simple question\nI asked ‚ÄúShow a table with names of each race in f1 2020 season?‚Äù\n\n\n\nRace data\n\n\nA quick look at wikipedia (https://en.wikipedia.org/wiki/2020_Formula_One_World_Championship) shows that this is accurate. This is a good step. Now, let‚Äôs add race-wise pitstop data to the table.\n\n\nAdd details\n\n\n\nRace data by pitstops\n\n\nOn the first look, this looks good. But, there is a problem. While the model generated reasonable statistics, they are not accurate. A quick check at Austrian Grand Prix and Styrian Grand Prix confirms that Lewis did not take 2 & 3 pitstops during the race.\nWhile this certainly looks like progress, we still have some way to go with factual accuracy. Let‚Äôs try a simpler question, instead.\n\n\nAdd more details\nI asked ChatGPT to add a column where Lewis finished each race.\n\n\n\nRace data with pitstops and finishing position\n\n\nA cursorily glance at the table shows that the model is getting the finishing position correct. But, the positions data is incorrect for 2 races. I‚Äôll leave it to you to figure out which ones! :-)"
  },
  {
    "objectID": "posts/2023-02-21-F1_Prompts/index.html#takeaways",
    "href": "posts/2023-02-21-F1_Prompts/index.html#takeaways",
    "title": "Generating sports statistics using ChatGPT",
    "section": "Takeaways",
    "text": "Takeaways\n\nWithout any coding and referencing multiple websites, I was able to pull this data. So, large language models like ChatGPT will be super useful in a wide variety of ways.\nWe still do not know the most appropriate ways to ask the model for the answer. We are figuring it out as we go.\nThe model is still not perfect. It is still making mistakes. But, it is getting better with time."
  }
]