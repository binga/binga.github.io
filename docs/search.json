[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Phani Srikanth",
    "section": "",
    "text": "testing."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to my home!",
    "section": "",
    "text": "üî≠ I‚Äôm currently working on helping my teammates succeed in protecting users from malicious file-based and web-based attacks using Machine Learning at Microsoft.\nüå± I‚Äôm learning how to lead great teams!\nüí¨ Ask me about Data Science / Building Career Paths / Storytelling / Interesting People on Internet / Systems Thinking.\nüì´ How to reach me: Twitter / LinkedIn / Microsoft\nüòÑ Pronouns: He/Him.\n‚ö° Fun fact: I never uttered a word until I turned 3.\n\nRead more about me here."
  },
  {
    "objectID": "recommendations.html",
    "href": "recommendations.html",
    "title": "Recommendations",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n¬†\n\n\n\n\n4 min\n\n\n\ndata science\n\n\ncompetitions\n\n\nmachine learning\n\n\nhackathons\n\n\n\n\n\n\n\nMar 30, 2017\n\n\n\n\n\n\n\n\n\n\n¬†\n\n\n\n\n2 min\n\n\n\ndata science\n\n\ncompetitions\n\n\nmachine learning\n\n\nhackathons\n\n\n\n\n\n\n\nDec 16, 2016\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2016-12-16-Winning_HackerEarth_ML_Challenge/index.html",
    "href": "posts/2016-12-16-Winning_HackerEarth_ML_Challenge/index.html",
    "title": "Winning the HackerEarth Machine Learning challenge",
    "section": "",
    "text": "Societe Generale, one of the largest banks in France, in collaboration with HackerEarth, organised Brainwaves, the annual hackathon at Bengaluru on November 12‚Äì13, 2016. The theme of the hackathon this year was ‚ÄúMachine Learning‚Äù. The hackathon had an online qualifier from where 85 top teams out of 2200 registrations from all over India, were selected for the final round. The final round was a 30-hour long hackathon which needed the teams to solve 1 problem out of 3 given problems spanning across transaction fraud detection, image and text analytics. I decided to solve the former since I have had experience working with banking data in multiple firms I have previously worked with."
  },
  {
    "objectID": "posts/2016-12-16-Winning_HackerEarth_ML_Challenge/index.html#brief-approach",
    "href": "posts/2016-12-16-Winning_HackerEarth_ML_Challenge/index.html#brief-approach",
    "title": "Winning the HackerEarth Machine Learning challenge",
    "section": "Brief Approach",
    "text": "Brief Approach\nFor the first problem, we were given millions of historical transactions to find patterns from and use these patterns to find anomalies on future transactions. We quickly skimmed through the data and built our machine learning model to predict the fraud on future transactional data and ranked #1 on the leaderboard. Eventually, we also built dashboards which can be used for proactive real-time monitoring for detection of any kind of new anomalies, or they can also be used to monitor transaction throughput etc. You could think of it like a one stop control centre with a global view of what‚Äôs going through the system. One commonly known fraudulent behaviour is, fraudsters try to exploit the system by doing high number of small debits and one large credit, thus swindling the money across countries and exchanges and hence trying to circumvent the defences of the system. This particular kind was quite challenging to incorporate into our machine learning model on and we are glad to have solved it to a good extent in 30 hours. Eventually, we had a good dashboard, a very good model and made an excellent pitch to the jury and ranked 1st amongst 85 teams."
  },
  {
    "objectID": "posts/2016-12-16-Winning_HackerEarth_ML_Challenge/index.html#experiences-at-the-hackathon",
    "href": "posts/2016-12-16-Winning_HackerEarth_ML_Challenge/index.html#experiences-at-the-hackathon",
    "title": "Winning the HackerEarth Machine Learning challenge",
    "section": "Experiences at the hackathon",
    "text": "Experiences at the hackathon\nThe hackathon was very well organised in terms of the quality of problem statements in the online and offline rounds, the way the organising team responded to any queries. It was genuinely surprising to see many mentors walking down to our table, talking to us about our backgrounds and providing us various domain related insights which augmented our model and resulted in higher performance. Even during the late hours none of them really left the place, they would always come and check if we were stalled anywhere and help us directionally so that we make constant progress. Having participated in a lot of hackathons prior to this one, I am very surprised by the energy levels of the mentors at Societe Generale.\nTo conclude, I would like to thank HackerEarth, Societe Generale, mentors and most importantly Phani Srinath and Supreeth Manyam for their fantastic work during the weekend. Great work guys! If not for any of the above, I am sure that weekend wouldn‚Äôt have been so memorable.\nAnd yes‚Ä¶ we partied long and hard that night! :-)\n  \nOur team with the amazing mentors and Societe Generale India CEO\nTop 3 teams pose for the customary picture. Overall, November 2016 has been a great month since Supreeth Manyam and I have also ended up winning another machine learning contest on CrowdAnalytix. We participated with the moniker Popeye For Olive!\nThat‚Äôs it for now. Stay tuned‚Ä¶"
  },
  {
    "objectID": "posts/2017-03-30-Winning_Two_ML_Challenges/index.html",
    "href": "posts/2017-03-30-Winning_Two_ML_Challenges/index.html",
    "title": "Winning two Machine Learning Challenges in the same month",
    "section": "",
    "text": "Analytics Roadshow MiniHack The first of these challenges was a mini-hack in which participants had three hours of time to build the most accurate predictive model that predicts surge pricing type for a taxi aggregator so that the matchmaking between the right cabs and right customers is quick and efficient.\nAfter a quick exploration of data, I encoded the Gender, Type of Cab and Confidence Life Style Index to numerical values so that tree models could consume the data. A 3-hour challenge doesn‚Äôt give you too much room to start with feature engineering. Hence, I directly jumped to building models to quickly test what works and what doesn‚Äôt. Linear models and Random forests produced around 0.63 and 0.68 on both cross-validation and leaderboard respectively. However, these stood around top 30th percentile. XGBoost, once again outperformed other models and it put me one among top 5 on leaderboard within an hour of the challenge. However, it was difficult to make any improvements to this.\n\nWhat didn‚Äôt work\n2-way variable interactions especially between var1, var2, var3 didn‚Äôt improve the score. Feature selection didn‚Äôt add any value. Ensembling RFs with XGBs produced no good effect. In a lot of ways, I was stuck for a long time. 30 minutes to finish line. I dropped to 15th position among 170 participants.\n\n\nWhat worked\nAs ensembling wasn‚Äôt working, I focused on improving my XGB model. I changed the min_child_weight parameter and voila! my score shot up. I was totally underfitting all the time, damn. I went up by 8 places. 15 minutes to go, I quickly lowered my learning_rate, my cross-validation score went up and it scored 2nd on LB! Falling short of time, I couldn‚Äôt add any other model. I was sure I didn‚Äôt overfit and I was delighted to see that my model was one of the most stable models on public and private and I scored 1st on private leaderboard.\nMaybe, it ain‚Äôt over until it‚Äôs over.\nRef: https://datahack.analyticsvidhya.com/contest/minihack-machine-learning/lb\nSource code here: https://github.com/binga/AnalyticsVidhya_Minihack\n2 weekends later..\n\n\nMLWare 2‚Äî Recommendation Challenge\nThis challenge was about building a model that predicts a given user‚Äôs ratings (from 0 to 10 stars) for a given item based on past ratings on other items and/or other information. The dataset approximately had one million data points and contained information about 40000 users, 120 items and the distribution of ratings was good enough to build a good recommendation engine.\n\nAs we were only given user_id, item_id and rating, there was no scope for feature engineering in this challenge. The data contained about a million data points and hence I split the entire dataset into a train and a validation set without any cross-validation loops. I started off with a collaborative filtering model in Python and a single CF model with 50 factors scored in the top 20%ile on leaderboard. Increasing or decreasing number of latent factors wasn‚Äôt contributing to an increase in score. However, ensembling 5 same kind of CF models with different random seed gave a significant boost, almost up to 3% and this model put me 2nd on the leaderboard, at the end of day one (in a 3-day challenge).\nUp next, I built a few LightGBM models to ensemble with the CF model. I used user-user features, user-item features like number of items rated by each user, {mean, median, max, min} rating by each user and all these features helped the LightGBM generalize well.\n\nWhat didn‚Äôt work\n\nXGBoost with the same features gave worser scores than LightGBM.\nAddition of item-item features didn‚Äôt contribute to the overall score.\nNeural network based collaborative filtering didn‚Äôt help improve the score.\n\n\n\nWhat worked\n\nSimple CF models and blended average.\nGBM models with user-item features.\n\nWith a weighted ensemble of CF model and LightGBM model, my model was good enough for a 2nd position on the leaderboard. On day 3 of the challenge, Rohan Rao who was 3rd on the leaderboard and I decided to team up. Rohan had multiple SVD models and a XGBoost stacker as his best model. This model‚Äôs score was close enough to my score but when we teamed up, our submissions correlation was 0.91 which was the most interesting thing. Immediately, we knew an average of both of our best models was going to do well and when we submitted this model, we zoomed to #1 position on the leaderboard. We tried a few things during the last hours of the competition but couldn‚Äôt improve the score.\nIn the end, we were glad to have maintained enough lead to remain #1 on the private leaderboard. Reference: https://datahack.analyticsvidhya.com/contest/mlware-2/lb\nThis marked the end of a good month where I ended up winning both the challenges I have taken part in. This is my 4th consecutive win in a hackathon. As I have lost one yesterday on HackerEarth, my good streak has finally come to an end and I decided to write down about the previous ones.\nWell, that‚Äôs it for today and I‚Äôm looking forward to do some more interesting and exciting work in the coming months.\nCheers!"
  }
]